{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: intro\n",
    "\n",
    "1. Make sure you know the basics\n",
    "2. train_test split\n",
    "3. using ML models, make predictions\n",
    "4. find the model with the least error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Ways to handle Missing Values\n",
    "1. drop columns\n",
    "2. imputation (filling), can add an extra column to identify filling\n",
    "\n",
    "\n",
    "# Exercise 1.\n",
    "1. Find ONLY missing column\n",
    "\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "miss_cols=missing_val_count_by_column[missing_val_count_by_column > 0]\n",
    "\n",
    "2. Get index of the missing columns\n",
    "miss_cols=miss_cols.index.tolist()\n",
    "\n",
    "3. Imputation (use imputer, only good at numerical data, fit transform for training, transform for valid)\n",
    "\n",
    "imputer=SimpleImputer(strategy='mean'/'median')\n",
    "\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "// put back columns to test accuracy\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "4. Preprocess data: creating a dataframe with no missing values\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Ways to handle categorical (nominal) variables\n",
    "\n",
    "1. Drop them, ordinal encoding (assigning each unique value to a different integer)\n",
    "2. One-hot encoding (create new columns with 0 and 1 to show yes or no of something)\n",
    "3. One-hot encoding > ordinal encoding > drop them\n",
    "\n",
    "# Exercise 2: Learned how to\n",
    "1. ordinal encoding only works when the categories are the SAME between train and valid\n",
    "2. How to identify categorical data and exclude them by\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "3. Apply encoder\n",
    "\n",
    "ordinal_encoder= OrdinalEncoder()\n",
    "\n",
    "label_X_train[good_label_cols]=ordinal_encoder.fit_transform(X_train[good_label_cols])\n",
    "\n",
    "label_X_valid[good_label_cols]=ordinal_encoder.transform(X_valid[good_label_cols])    \n",
    "\n",
    "4. Cardinality: low cardinaltiy for one-hot encoding\n",
    "\n",
    "5. One-hot encoder\n",
    "\n",
    "OH_encoder=OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "// Preprocessing with only the low cardinality data\n",
    "\n",
    "OH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "\n",
    "OH_X_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "\n",
    "// One-hot encoding removed index, put it back\n",
    "\n",
    "OH_X_train.index=X_train.index\n",
    "\n",
    "OH_X_valid.index=X_valid.index\n",
    "\n",
    "\n",
    "// Remove categorical columns (will be replaced with one-hot encoding)\n",
    "\n",
    "num_X_train=X_train.drop(object_cols, axis=1)\n",
    "\n",
    "num_X_valid=X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "\n",
    "// Add one-hot encoded columns to numerical features\n",
    "\n",
    "OH_X_train=pd.concat([num_X_train, OH_X_train], axis=1)\n",
    "\n",
    "OH_X_valid=pd.concat([num_X_valid, OH_X_valid], axis=1)\n",
    "\n",
    "\n",
    "// Ensure all columns have string type\n",
    "\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Pipelines\n",
    "\n",
    "1. Wrapper that wrap data preprocessing (imputation and encoding on data), derived as preprocessor\n",
    "2. Apply pipeline with model and a preprocessor\n",
    "3. make predictions and evaluate scores\n",
    "\n",
    "## Exercise 4\n",
    "1. Learn how to bundle things together\n",
    "2. Use ColumnTransformer to transform numerical and categorical data, then assign it to preprocessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Cross Validation\n",
    "1. divide data into folds, then use different fold as validation every time to fit the model to measure a model's performance\n",
    "2. cross_val_score returns negative MAE score, recommend to use multiple folds to improve scores\n",
    "3. Help tune hyperparameters\n",
    "\n",
    "## Exercise 5:\n",
    "1. Use pipeline (preprocessor and model) to calculate different scores\n",
    "2. Learned to tune hyperparameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Gradient Boosting\n",
    "1. Add different models iteratively \n",
    "2. Ensemble methods (combine predictions of several models ex. multiple tries)\n",
    "3. Parameter tuning to lower the MAE\n",
    "\n",
    "\n",
    "## Steps of a Gradient Boosting\n",
    "1. start with a naive model\n",
    "2. make predictions with it\n",
    "3. Calcualte loss\n",
    "4. Train newer model\n",
    "5. Add newer model\n",
    "6. Make predictions again"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
