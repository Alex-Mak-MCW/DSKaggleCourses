{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "1. Feature Engineering: simply to make your data better suited to the problem at hand\n",
    "2. It can improve a model's performance\n",
    "3. It can reduce computational or data needs\n",
    "4. It can improve interpretability of the results\n",
    "5. Feature is useful when it has a relationship\n",
    "\n",
    "## Mutual Information (MI)\n",
    "1. Building a feature utility metric to find each feature's association with its target\n",
    "2. Finding the mutual dependence of 2 features\n",
    "3. Mutual information is a great general-purpose metric\n",
    "4. Mutual information is easy to use and interpret, efficient, resistant to overfitting \n",
    "5. MI describes relationship in terms of uncertainty (how one feature affect other feature's uncertainty)\n",
    "6. When MI=0, the quanitites are independent\n",
    "7. There is no upperbound in MI\n",
    "8. from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "\n",
    "## Creating Features\n",
    "1. Mathematical transformation: making new features based on math from 2 featues (+ or - or * or /)\n",
    "2. One Hot Encoding with categorical features\n",
    "3. Counts from binary or boolean\n",
    "4. Building-Up and Breaking-Down Features with string addition/ string slicing\n",
    "5. Group Transform: transform a value that grouped features together\n",
    "\n",
    "## Clustering with K-means (for unsupervised learning algorithm)\n",
    "1. clustering: assign datapoints based on how similar the points are to each other\n",
    "2. K-means clustering: measures similairty based on euclidean distance, creates a cluster of points (centroids), k= # of centroids\n",
    "3. Voronoi tessallation: when a centroid captures points overlap with another centroid\n",
    "4. arguments: n_clusters(# of centroids), max_iter(# of iter to get the shortest total distance between each point and centroid), n_init(# of times K-means repeats)\n",
    "5. Applitcation : X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "1. Instead of describing the data with the original features, we describe it with its axes of variation\n",
    "2. These new features will be called \"Principal Component\", their weights are called as \"loadings\"\n",
    "3. Number of features = Number of Principal Component\n",
    "4. Show amount of variation: percent of explained variance\n",
    "5. 2 uses in Feature Engineering: descriptive technique, use components as features\n",
    "6. Benefits: Dimensionality reeduction, anomaly (Irregularities) reduction, noise reduction, decorrelation\n",
    "\n",
    "### Applitcation of PCA:\n",
    "1. pca = PCA(), fit transform fitst\n",
    "2. X_pca = pca.fit_transform(X_scaled), then convert to a dataframe\n",
    "3. component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "4. X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "## Target encoding\n",
    "1. Replaces a feature's categories with some number derived from the target\n",
    "2. mean encoding (apply mean)/ binary encoding (apply binary)\n",
    "3. Creates risk of overfitting, solution: smoothing (blend the in-category avg with overall avg)\n",
    "4. smoothing psuedocode: encoding = weight * in_category + (1 - weight) * overall\n",
    "5. Way to determine weight : m-estimate (weight = n / (n + m)), n=total number of times that category occurs in the data, m=\"smoothing factor\"\n",
    "6. Application cases: on high cardinality(# of categories) features, domain-motivated features\n",
    "7. XGBoost was able to get an almost a perfect fit after mean-encoding the count feature\n",
    "8. Always use separate data sets for training the encoder and training the model\n",
    "\n",
    "### Application of target encoding\n",
    "1. import it: \n",
    "from category_encoders import MEstimateEncoder\n",
    "2. Create the encoder instance. Choose m to control noise:\n",
    "encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "3. Fit the encoder on the encoding split:\n",
    "encoder.fit(X_encode, y_encode)\n",
    "4. Encode the Zipcode column to create the final training data\n",
    "X_train = encoder.transform(X_pretrain)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
